{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-mNJaVIot65Y"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"31xubTdPSOp5"},"source":["# Pre-processing data"]},{"cell_type":"markdown","metadata":{"id":"nuWy5rxst65d"},"source":["## read data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3sVbFJVrIwIf"},"outputs":[],"source":["#load data\n","def read_data(file_path):\n","    data = np.genfromtxt(file_path, dtype='str')\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"WtiIbsy5bmGL"},"source":["## split data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7n5o5lW4t65e"},"outputs":[],"source":["#split data\n","\n","def train_test_split(X, y, test_ratio, random_seed=None):\n","    if random_seed is not None:\n","        np.random.seed(random_seed)\n","\n","    # Shuffle the indices\n","    num_samples = len(X)\n","    shuffled_indices = np.random.permutation(num_samples)\n","\n","    # Calculate the number of samples for the test set\n","    num_test_samples = int(test_ratio * num_samples)\n","\n","    # Split the data\n","    test_indices = shuffled_indices[:num_test_samples]\n","    train_indices = shuffled_indices[num_test_samples:]\n","\n","    X_train, X_test = X[train_indices], X[test_indices]\n","    y_train, y_test = y[train_indices], y[test_indices]\n","\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"markdown","metadata":{"id":"quae_nkbSvm6"},"source":["## Oversampling (smote)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6rLl9mDt65f"},"outputs":[],"source":["# oversample data\n","def oversample_data(X, y):\n","\n","    # Identify minority and majority class\n","    unique_classes, class_counts = np.unique(y, return_counts=True)\n","    minority_class = unique_classes[np.argmin(class_counts)]\n","    majority_class = unique_classes[np.argmax(class_counts)]\n","\n","    # Find indices of minority class samples\n","    minority_indices = np.where(y == minority_class)[0]\n","\n","    # Calculate the oversampling factor\n","    oversample_factor = int(class_counts[0] / class_counts[1]) - 1\n","\n","    # Oversample the minority class by duplicating samples\n","    oversampled_indices = np.tile(minority_indices, oversample_factor)\n","    oversampled_X = np.concatenate([X, X[oversampled_indices]])\n","    oversampled_y = np.concatenate([y, y[oversampled_indices]])\n","\n","    # Shuffle the oversampled data\n","    shuffle_indices = np.random.permutation(len(oversampled_X))\n","    oversampled_X = oversampled_X[shuffle_indices]\n","    oversampled_y = oversampled_y[shuffle_indices]\n","\n","    return oversampled_X, oversampled_y\n"]},{"cell_type":"markdown","metadata":{"id":"QKLTZdGYSXTg"},"source":["## Label transform"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUbns4l1t65g"},"outputs":[],"source":["# label transform -1 to 0 1 to 1\n","def transform_labels(labels):\n","    return (np.array(labels, dtype=int) + 1) // 2"]},{"cell_type":"markdown","metadata":{"id":"iS03k8q_t65g"},"source":["## Extracting features using K-mer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-6RDXzct65g"},"outputs":[],"source":["# extracting feature for all train data, val data and test data\n","# feature extrac using k-mer model\n","def k_mer(train_peptides, val_peptides, test_peptides, k=3):\n","    def generate_kmers(peptides, k):\n","        return [peptides[i:i + k] for i in range(len(peptides) - k + 1)]\n","\n","    def create_vocabulary(data, k):\n","        kmers = set()\n","        for sequence in data:\n","            kmers.update(generate_kmers(sequence, k))\n","        return sorted(list(kmers))\n","\n","    def kmer_encoded_peptide(data, vocabulary, k):\n","        kmer_rep = np.zeros((len(data), len(vocabulary)))\n","        for i, sequence in enumerate(data):\n","            for kmer in generate_kmers(sequence, k):\n","                if kmer in vocabulary:\n","                    kmer_rep[i, vocabulary.index(kmer)] += 1\n","        return kmer_rep\n","\n","    # Create k-mer vocabulary from the training set\n","    kmer_vocabulary = create_vocabulary(train_peptides, k)\n","\n","    # Generate k-mer representation for training and validation sets\n","    train_kmer = kmer_encoded_peptide(train_peptides, kmer_vocabulary, k)\n","    val_kmer = kmer_encoded_peptide(val_peptides, kmer_vocabulary, k)\n","    test_kmer = kmer_encoded_peptide(test_peptides, kmer_vocabulary, k)\n","\n","    return train_kmer, val_kmer, test_kmer\n"]},{"cell_type":"markdown","metadata":{"id":"kFhO_a3Vt65h"},"source":["## Get the final train data, val data and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YsePM3Vet65h"},"outputs":[],"source":["# load data\n","train = read_data('train.dat')\n","test = read_data('test.dat')\n","X = train[:, 1]\n","y = train[:, 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6iRevQ_t65h","outputId":"95d835af-09a5-4746-e545-f183761fc197"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1253,) (313,) (1253,) (313,)\n"]}],"source":["# split the train data to train data and val data\n","test_ratio=0.2\n","X_train, X_val, y_train, y_val = train_test_split(X,y, test_ratio)\n","print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMYpoGVet65i"},"outputs":[],"source":["# oversampled the train data\n","oversampled_X, oversampled_y = oversample_data(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_qv9X6Ft65i"},"outputs":[],"source":["# train data and val data label transform -1 to 0, 1 to 1\n","oversampled_y_labels = transform_labels(oversampled_y)\n","y_val_labels = transform_labels(y_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-xOYbzZt65i"},"outputs":[],"source":["# extract feature datas\n","#X_train_bows, X_val_bows, X_test_bows = bag_of_words(oversampled_X, X_val, test)\n","X_train_kmer, X_val_kmer, X_test_kmer = k_mer(oversampled_X, X_val, test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TlXNfp2t65j","outputId":"a6200769-76b3-4483-d2e8-65cd6dc8f068"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2181, 6517) (313, 6517) (2181,) (313,) (392, 6517)\n"]}],"source":["#print the shape for all data\n","print(X_train_kmer.shape, X_val_kmer.shape, oversampled_y_labels.shape,y_val_labels.shape ,X_test_kmer.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSB07z84t65j"},"outputs":[],"source":["#transpose x_train\n","X_train_features = X_train_kmer.T\n","# reshape the label\n","y_train_labels = oversampled_y_labels[np.newaxis]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmkjgdb0t65j","outputId":"675373f0-586f-437b-ba10-44ae0f4b3c40"},"outputs":[{"name":"stdout","output_type":"stream","text":["(6517, 2181) (1, 2181)\n"]}],"source":["print(X_train_features.shape, y_train_labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"gDHc8nso95wG"},"source":["# Model\n","one input layer --one hidden layer -- one output layer"]},{"cell_type":"markdown","metadata":{"id":"yQvgqt9rAsML"},"source":["# Activation Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zq9T-tv7A7jO"},"outputs":[],"source":["def tanh(x):\n","    return np.tanh(x)\n","\n","def relu(x):\n","    return np.maximum(x, 0)\n","\n","def sigmoid(x):\n","    return 1/(1+np.exp(-x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HsC7uazWBA6a"},"outputs":[],"source":["def derivative_tanh(x):\n","    return (1 - np.power(np.tanh(x), 2))\n","\n","def derivative_relu(x):\n","    return np.array(x > 0, dtype = np.float32)"]},{"cell_type":"markdown","metadata":{"id":"EaAUlaYD99lB"},"source":["#Initialize parameters Randomly\n","W1=np.random.randn(n1,n0)\n","\n","b1=np.zeros((n1,1))\n","\n","W2=np.random.randn(n2,n1)\n","\n","b2=np.zeros((n2,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5P10bJ7--DCb"},"outputs":[],"source":["def initialize_parameters(n_x, n_h, n_y):\n","    w1 = np.random.randn(n_h, n_x)*0.1\n","    b1 = np.zeros((n_h, 1))\n","\n","    w2 = np.random.randn(n_y, n_h)*0.1\n","    b2 = np.zeros((n_y, 1))\n","\n","    parameters = {\n","        \"w1\" : w1,\n","        \"b1\" : b1,\n","        \"w2\" : w2,\n","        \"b2\" : b2\n","    }\n","\n","    return parameters"]},{"cell_type":"markdown","metadata":{"id":"UWfrKPZX-okV"},"source":["# Forward Propagation\n","\n","$ Z_1 = W_1 * X + B_1 $\n","\n","$ A_1 = f ( Z_1 ) $  \n","\n","$ Z_2 = W2 * A_1 + B_2 $\n","\n","$ A_2 = sigmoid( Z_2 ) $\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKsuWC_f-trH"},"outputs":[],"source":["def forward_propagation(x, parameters):\n","\n","    w1 = parameters['w1']\n","    b1 = parameters['b1']\n","    w2 = parameters['w2']\n","    b2 = parameters['b2']\n","\n","    z1 = np.dot(w1, x) + b1\n","    a1 = relu(z1)\n","\n","    z2 = np.dot(w2, a1) + b2\n","    a2 = sigmoid(z2)\n","\n","    forward_cache = {\n","        \"z1\" : z1,\n","        \"a1\" : a1,\n","        \"z2\" : z2,\n","        \"a2\" : a2\n","    }\n","\n","    return forward_cache"]},{"cell_type":"markdown","metadata":{"id":"6tE89JFg_RBh"},"source":["## Cost Function\n","\n","$ Cost = - \\frac{1}{m} \\sum_{i=1}^{m} [ y*log(a_i) + (1-y)*log(1 - a_i) ] $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLVJi9UB_SLa"},"outputs":[],"source":["def cost_function(a2, y):\n","    m = y.shape[1]\n","\n","    cost = (1./m) * (-np.dot(y,np.log(a2).T) - np.dot(1-y, np.log(1-a2).T))\n","    cost = np.squeeze(cost)\n","\n","    return cost"]},{"cell_type":"markdown","metadata":{"id":"PDv5RQho_HLq"},"source":["# Backward Propagation\n","dZ2=(A2−Y)\n","\n","dW2=1/m.dZ2.AT1\n","\n","dB2=1/m.sum(dZ2,1)\n","\n","dZ1=WT2.dZ2∗f|1(Z1)\n","\n","dW1=1/m.dZ1.XT\n","\n","dB1=1/m.sum(dZ1,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPUxKf5E_bHD"},"outputs":[],"source":["def backward_prop(x, y, parameters, forward_cache):\n","\n","    w1 = parameters['w1']\n","    b1 = parameters['b1']\n","    w2 = parameters['w2']\n","    b2 = parameters['b2']\n","\n","    a1 = forward_cache['a1']\n","    a2 = forward_cache['a2']\n","\n","    m = x.shape[1]\n","\n","    dz2 = (a2 - y)\n","    dw2 = (1/m)*np.dot(dz2, a1.T)\n","    db2 = (1/m)*np.sum(dz2, axis = 1, keepdims = True)\n","\n","    dz1 = (1/m)*np.dot(w2.T, dz2)*derivative_relu(a1)\n","    dw1 = (1/m)*np.dot(dz1, x.T)\n","    db1 = (1/m)*np.sum(dz1, axis = 1, keepdims = True)\n","\n","    gradients = {\n","        \"dw1\" : dw1,\n","        \"db1\" : db1,\n","        \"dw2\" : dw2,\n","        \"db2\" : db2\n","    }\n","\n","    return gradients"]},{"cell_type":"markdown","metadata":{"id":"Uh1V-TI4_cGb"},"source":["# Updating Parameters\n","\n","$ W_2 = W_2 -  \\alpha * \\frac{\\partial Cost }{\\partial W_2}$\n","\n","$ B_2 = B_2 -  \\alpha * \\frac{\\partial Cost }{\\partial B_2}$\n","\n","$ W_1 = W_1 -  \\alpha * \\frac{\\partial Cost }{\\partial W_1}$\n","\n","$ B_1 = B_1 -  \\alpha * \\frac{\\partial Cost }{\\partial B_1}$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9CYF43G_r06"},"outputs":[],"source":["def update_parameters(parameters, gradients, learning_rate):\n","\n","    w1 = parameters['w1']\n","    b1 = parameters['b1']\n","    w2 = parameters['w2']\n","    b2 = parameters['b2']\n","\n","    dw1 = gradients['dw1']\n","    db1 = gradients['db1']\n","    dw2 = gradients['dw2']\n","    db2 = gradients['db2']\n","\n","    w1 = w1 - learning_rate*dw1\n","    b1 = b1 - learning_rate*db1\n","    w2 = w2 - learning_rate*dw2\n","    b2 = b2 - learning_rate*db2\n","\n","    parameters = {\n","        \"w1\" : w1,\n","        \"b1\" : b1,\n","        \"w2\" : w2,\n","        \"b2\" : b2\n","    }\n","\n","    return parameters"]},{"cell_type":"markdown","metadata":{"id":"lVi6BE3Q_t4k"},"source":["# Complete Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oj2MmOHU_wv6"},"outputs":[],"source":["def model(x, y, n_h, learning_rate, iterations):\n","\n","    n_x = x.shape[0]\n","    n_y = y.shape[0]\n","\n","    cost_list = []\n","\n","    parameters = initialize_parameters(n_x, n_h, n_y)\n","\n","    for i in range(iterations):\n","\n","        forward_cache = forward_propagation(x, parameters)\n","\n","        cost = cost_function(forward_cache['a2'], y)\n","\n","        gradients = backward_prop(x, y, parameters, forward_cache)\n","\n","        parameters = update_parameters(parameters, gradients, learning_rate)\n","\n","        cost_list.append(cost)\n","\n","        if(i%(iterations/10) == 0):\n","            print(\"Cost after\", i, \"iterations is :\", cost)\n","\n","    return parameters, cost_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0MGnHQZ_0Zf","outputId":"f98d4667-3059-4f46-8d2e-2cb8b01e0033"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cost after 0 iterations is : 0.799879512413404\n","Cost after 10 iterations is : 0.9469356824635891\n","Cost after 20 iterations is : 0.3407856158093043\n","Cost after 30 iterations is : 0.22240137461748474\n","Cost after 40 iterations is : 0.19080481293154122\n","Cost after 50 iterations is : 0.16856141006344774\n","Cost after 60 iterations is : 0.15166894249461255\n","Cost after 70 iterations is : 0.1382862128896168\n","Cost after 80 iterations is : 0.12735361599865988\n","Cost after 90 iterations is : 0.11821309978324872\n"]}],"source":["iterations = 100\n","n_h = 1000\n","learning_rate = 0.5\n","Parameters, Cost_list = model(X_train_features, y_train_labels, n_h = n_h, learning_rate = learning_rate, iterations = iterations)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xswz7Xkht65m"},"outputs":[],"source":["# calculate mcc\n","def calculate_mcc(predicted_labels, true_labels):\n","    if len(predicted_labels) != len(true_labels):\n","        raise ValueError(\"Lengths of predicted_labels and true_labels must be the same.\")\n","\n","    TP, TN, FP, FN = 0, 0, 0, 0\n","\n","    for predicted, true in zip(predicted_labels, true_labels):\n","        if predicted == 1 and true == 1:\n","            TP += 1\n","        elif predicted == 0 and true == 0:\n","            TN += 1\n","        elif predicted == 1 and true == 0:\n","            FP += 1\n","        elif predicted == 0 and true == 1:\n","            FN += 1\n","\n","    numerator = TP * TN - FP * FN\n","    denominator = ((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n","\n","    if denominator == 0:\n","        return 0  # Handle division by zero\n","    else:\n","        return numerator / (denominator ** 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDEJe2Grt65q"},"outputs":[],"source":["def accuracy(X, y, parameters):\n","  m = X.shape[1]\n","  forward_cache = forward_propagation(X, parameters)\n","  a_out = forward_cache['a2']\n","  y_pred = np.array(a_out > 0.5, dtype = 'float')\n","  acc = np.mean(y_pred == y)*100\n","  return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htiKP0ZTt65q","outputId":"8aed8af5-ddf4-43d3-e04e-b3b2cb5c1b4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of Train Dataset: 99.40394314534618\n","Accuracy of Val Dataset: 97.76357827476039\n"]}],"source":["print(\"Accuracy of Train Dataset:\", accuracy(X_train_features, y_train_labels, Parameters))\n","print(\"Accuracy of Val Dataset:\", accuracy(X_val_kmer.T, y_val_labels, Parameters))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdHfSCpqt65r"},"outputs":[],"source":["def predict(X):\n","    m = X.T.shape[1]\n","    forward_cache = forward_propagation(X.T, Parameters)\n","    a_out = forward_cache['a2']\n","    y_pred = np.array(a_out > 0.5, dtype = 'float')\n","    y_pred[y_pred==0]=-1\n","    return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4giDMJ4Ut65r"},"outputs":[],"source":["y_test_predict = predict(X_test_kmer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33qinSbOt65r"},"outputs":[],"source":["f=open('results_kmer.txt','w')\n","for i in y_test_predict.T:\n","    f.write(str(int(i))+'\\n')\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ASojUOZt65r"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}